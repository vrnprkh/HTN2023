{"ast":null,"code":"// File generated from our OpenAPI spec by Stainless.\nimport { APIResource } from 'openai/resource';\nexport class Completions extends APIResource {\n  create(body, options) {\n    var _a;\n    return this.post('/chat/completions', {\n      body,\n      ...options,\n      stream: (_a = body.stream) !== null && _a !== void 0 ? _a : false\n    });\n  }\n}\n(function (Completions) {})(Completions || (Completions = {}));","map":{"version":3,"names":["APIResource","Completions","create","body","options","post","stream","_a"],"sources":["/home/zain/HTN2023/node_modules/openai/src/resources/chat/completions.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from '../../core.js';\nimport { APIPromise } from '../../core.js';\nimport { APIResource } from '../../resource.js';\nimport * as Completions_ from '../completions.js';\nimport * as API from './index.js';\nimport { Stream } from '../../streaming.js';\n\nexport class Completions extends APIResource {\n  /**\n   * Creates a model response for the given chat conversation.\n   */\n  create(\n    body: ChatCompletionCreateParamsNonStreaming,\n    options?: Core.RequestOptions,\n  ): APIPromise<ChatCompletion>;\n  create(\n    body: ChatCompletionCreateParamsStreaming,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<ChatCompletionChunk>>;\n  create(\n    body: ChatCompletionCreateParamsBase,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<ChatCompletionChunk> | ChatCompletion>;\n  create(\n    body: ChatCompletionCreateParams,\n    options?: Core.RequestOptions,\n  ): APIPromise<ChatCompletion> | APIPromise<Stream<ChatCompletionChunk>> {\n    return this.post('/chat/completions', { body, ...options, stream: body.stream ?? false }) as\n      | APIPromise<ChatCompletion>\n      | APIPromise<Stream<ChatCompletionChunk>>;\n  }\n}\n\n/**\n * Represents a chat completion response returned by model, based on the provided\n * input.\n */\nexport interface ChatCompletion {\n  /**\n   * A unique identifier for the chat completion.\n   */\n  id: string;\n\n  /**\n   * A list of chat completion choices. Can be more than one if `n` is greater\n   * than 1.\n   */\n  choices: Array<ChatCompletion.Choice>;\n\n  /**\n   * The Unix timestamp (in seconds) of when the chat completion was created.\n   */\n  created: number;\n\n  /**\n   * The model used for the chat completion.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always `chat.completion`.\n   */\n  object: string;\n\n  /**\n   * Usage statistics for the completion request.\n   */\n  usage?: Completions_.CompletionUsage;\n}\n\nexport namespace ChatCompletion {\n  export interface Choice {\n    /**\n     * The reason the model stopped generating tokens. This will be `stop` if the model\n     * hit a natural stop point or a provided stop sequence, `length` if the maximum\n     * number of tokens specified in the request was reached, or `function_call` if the\n     * model called a function.\n     */\n    finish_reason: 'stop' | 'length' | 'function_call';\n\n    /**\n     * The index of the choice in the list of choices.\n     */\n    index: number;\n\n    /**\n     * A chat completion message generated by the model.\n     */\n    message: ChatCompletionMessage;\n  }\n}\n\n/**\n * Represents a streamed chunk of a chat completion response returned by model,\n * based on the provided input.\n */\nexport interface ChatCompletionChunk {\n  /**\n   * A unique identifier for the chat completion chunk.\n   */\n  id: string;\n\n  /**\n   * A list of chat completion choices. Can be more than one if `n` is greater\n   * than 1.\n   */\n  choices: Array<ChatCompletionChunk.Choice>;\n\n  /**\n   * The Unix timestamp (in seconds) of when the chat completion chunk was created.\n   */\n  created: number;\n\n  /**\n   * The model to generate the completion.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always `chat.completion.chunk`.\n   */\n  object: string;\n}\n\nexport namespace ChatCompletionChunk {\n  export interface Choice {\n    /**\n     * A chat completion delta generated by streamed model responses.\n     */\n    delta: Choice.Delta;\n\n    /**\n     * The reason the model stopped generating tokens. This will be `stop` if the model\n     * hit a natural stop point or a provided stop sequence, `length` if the maximum\n     * number of tokens specified in the request was reached, or `function_call` if the\n     * model called a function.\n     */\n    finish_reason: 'stop' | 'length' | 'function_call' | null;\n\n    /**\n     * The index of the choice in the list of choices.\n     */\n    index: number;\n  }\n\n  export namespace Choice {\n    /**\n     * A chat completion delta generated by streamed model responses.\n     */\n    export interface Delta {\n      /**\n       * The contents of the chunk message.\n       */\n      content?: string | null;\n\n      /**\n       * The name and arguments of a function that should be called, as generated by the\n       * model.\n       */\n      function_call?: Delta.FunctionCall;\n\n      /**\n       * The role of the author of this message.\n       */\n      role?: ChatCompletionRole;\n    }\n\n    export namespace Delta {\n      /**\n       * The name and arguments of a function that should be called, as generated by the\n       * model.\n       */\n      export interface FunctionCall {\n        /**\n         * The arguments to call the function with, as generated by the model in JSON\n         * format. Note that the model does not always generate valid JSON, and may\n         * hallucinate parameters not defined by your function schema. Validate the\n         * arguments in your code before calling your function.\n         */\n        arguments?: string;\n\n        /**\n         * The name of the function to call.\n         */\n        name?: string;\n      }\n    }\n  }\n}\n\n/**\n * A chat completion message generated by the model.\n */\nexport interface ChatCompletionMessage {\n  /**\n   * The contents of the message.\n   */\n  content: string | null;\n\n  /**\n   * The role of the author of this message.\n   */\n  role: ChatCompletionRole;\n\n  /**\n   * The name and arguments of a function that should be called, as generated by the\n   * model.\n   */\n  function_call?: ChatCompletionMessage.FunctionCall;\n}\n\nexport namespace ChatCompletionMessage {\n  /**\n   * The name and arguments of a function that should be called, as generated by the\n   * model.\n   */\n  export interface FunctionCall {\n    /**\n     * The arguments to call the function with, as generated by the model in JSON\n     * format. Note that the model does not always generate valid JSON, and may\n     * hallucinate parameters not defined by your function schema. Validate the\n     * arguments in your code before calling your function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n}\n\nexport interface ChatCompletionMessageParam {\n  /**\n   * The contents of the message. `content` is required for all messages, and may be\n   * null for assistant messages with function calls.\n   */\n  content: string | null;\n\n  /**\n   * The role of the messages author. One of `system`, `user`, `assistant`, or\n   * `function`.\n   */\n  role: 'system' | 'user' | 'assistant' | 'function';\n\n  /**\n   * The name and arguments of a function that should be called, as generated by the\n   * model.\n   */\n  function_call?: ChatCompletionMessageParam.FunctionCall;\n\n  /**\n   * The name of the author of this message. `name` is required if role is\n   * `function`, and it should be the name of the function whose response is in the\n   * `content`. May contain a-z, A-Z, 0-9, and underscores, with a maximum length of\n   * 64 characters.\n   */\n  name?: string;\n}\n\nexport namespace ChatCompletionMessageParam {\n  /**\n   * The name and arguments of a function that should be called, as generated by the\n   * model.\n   */\n  export interface FunctionCall {\n    /**\n     * The arguments to call the function with, as generated by the model in JSON\n     * format. Note that the model does not always generate valid JSON, and may\n     * hallucinate parameters not defined by your function schema. Validate the\n     * arguments in your code before calling your function.\n     */\n    arguments: string;\n\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n}\n\n/**\n * The role of the author of this message.\n */\nexport type ChatCompletionRole = 'system' | 'user' | 'assistant' | 'function';\n\n/**\n * @deprecated ChatCompletionMessageParam should be used instead\n */\nexport type CreateChatCompletionRequestMessage = ChatCompletionMessageParam;\n\nexport type ChatCompletionCreateParams =\n  | ChatCompletionCreateParamsNonStreaming\n  | ChatCompletionCreateParamsStreaming;\n\nexport interface ChatCompletionCreateParamsBase {\n  /**\n   * A list of messages comprising the conversation so far.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).\n   */\n  messages: Array<ChatCompletionMessageParam>;\n\n  /**\n   * ID of the model to use. See the\n   * [model endpoint compatibility](https://platform.openai.com/docs/models/model-endpoint-compatibility)\n   * table for details on which models work with the Chat API.\n   */\n  model:\n    | (string & {})\n    | 'gpt-4'\n    | 'gpt-4-0314'\n    | 'gpt-4-0613'\n    | 'gpt-4-32k'\n    | 'gpt-4-32k-0314'\n    | 'gpt-4-32k-0613'\n    | 'gpt-3.5-turbo'\n    | 'gpt-3.5-turbo-16k'\n    | 'gpt-3.5-turbo-0301'\n    | 'gpt-3.5-turbo-0613'\n    | 'gpt-3.5-turbo-16k-0613';\n\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their\n   * existing frequency in the text so far, decreasing the model's likelihood to\n   * repeat the same line verbatim.\n   *\n   * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/gpt/parameter-details)\n   */\n  frequency_penalty?: number | null;\n\n  /**\n   * Controls how the model responds to function calls. \"none\" means the model does\n   * not call a function, and responds to the end-user. \"auto\" means the model can\n   * pick between an end-user or calling a function. Specifying a particular function\n   * via `{\"name\": \"my_function\"}` forces the model to call that function. \"none\" is\n   * the default when no functions are present. \"auto\" is the default if functions\n   * are present.\n   */\n  function_call?: 'none' | 'auto' | ChatCompletionCreateParams.FunctionCallOption;\n\n  /**\n   * A list of functions the model may generate JSON inputs for.\n   */\n  functions?: Array<ChatCompletionCreateParams.Function>;\n\n  /**\n   * Modify the likelihood of specified tokens appearing in the completion.\n   *\n   * Accepts a json object that maps tokens (specified by their token ID in the\n   * tokenizer) to an associated bias value from -100 to 100. Mathematically, the\n   * bias is added to the logits generated by the model prior to sampling. The exact\n   * effect will vary per model, but values between -1 and 1 should decrease or\n   * increase likelihood of selection; values like -100 or 100 should result in a ban\n   * or exclusive selection of the relevant token.\n   */\n  logit_bias?: Record<string, number> | null;\n\n  /**\n   * The maximum number of [tokens](/tokenizer) to generate in the chat completion.\n   *\n   * The total length of input tokens and generated tokens is limited by the model's\n   * context length.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)\n   * for counting tokens.\n   */\n  max_tokens?: number;\n\n  /**\n   * How many chat completion choices to generate for each input message.\n   */\n  n?: number | null;\n\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on\n   * whether they appear in the text so far, increasing the model's likelihood to\n   * talk about new topics.\n   *\n   * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/gpt/parameter-details)\n   */\n  presence_penalty?: number | null;\n\n  /**\n   * Up to 4 sequences where the API will stop generating further tokens.\n   */\n  stop?: string | null | Array<string>;\n\n  /**\n   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).\n   */\n  stream?: boolean | null;\n\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will\n   * make the output more random, while lower values like 0.2 will make it more\n   * focused and deterministic.\n   *\n   * We generally recommend altering this or `top_p` but not both.\n   */\n  temperature?: number | null;\n\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the\n   * model considers the results of the tokens with top_p probability mass. So 0.1\n   * means only the tokens comprising the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or `temperature` but not both.\n   */\n  top_p?: number | null;\n\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor\n   * and detect abuse.\n   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).\n   */\n  user?: string;\n}\n\nexport namespace ChatCompletionCreateParams {\n  export interface FunctionCallOption {\n    /**\n     * The name of the function to call.\n     */\n    name: string;\n  }\n\n  export interface Function {\n    /**\n     * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain\n     * underscores and dashes, with a maximum length of 64.\n     */\n    name: string;\n\n    /**\n     * The parameters the functions accepts, described as a JSON Schema object. See the\n     * [guide](https://platform.openai.com/docs/guides/gpt/function-calling) for\n     * examples, and the\n     * [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for\n     * documentation about the format.\n     *\n     * To describe a function that accepts no parameters, provide the value\n     * `{\"type\": \"object\", \"properties\": {}}`.\n     */\n    parameters: Record<string, unknown>;\n\n    /**\n     * A description of what the function does, used by the model to choose when and\n     * how to call the function.\n     */\n    description?: string;\n  }\n\n  export type ChatCompletionCreateParamsNonStreaming = API.ChatCompletionCreateParamsNonStreaming;\n  export type ChatCompletionCreateParamsStreaming = API.ChatCompletionCreateParamsStreaming;\n}\n\n/**\n * @deprecated Use ChatCompletionCreateParams instead\n */\nexport type CompletionCreateParams = ChatCompletionCreateParams;\n\nexport interface ChatCompletionCreateParamsNonStreaming extends ChatCompletionCreateParamsBase {\n  /**\n   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).\n   */\n  stream?: false | null;\n}\n\n/**\n * @deprecated Use ChatCompletionCreateParamsNonStreaming instead\n */\nexport type CompletionCreateParamsNonStreaming = ChatCompletionCreateParamsNonStreaming;\n\nexport interface ChatCompletionCreateParamsStreaming extends ChatCompletionCreateParamsBase {\n  /**\n   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]`\n   * message.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).\n   */\n  stream: true;\n}\n\n/**\n * @deprecated Use ChatCompletionCreateParamsStreaming instead\n */\nexport type CompletionCreateParamsStreaming = ChatCompletionCreateParamsStreaming;\n\nexport namespace Completions {\n  export import ChatCompletion = API.ChatCompletion;\n  export import ChatCompletionChunk = API.ChatCompletionChunk;\n  export import ChatCompletionMessage = API.ChatCompletionMessage;\n  export import ChatCompletionMessageParam = API.ChatCompletionMessageParam;\n  export import ChatCompletionRole = API.ChatCompletionRole;\n  export import CreateChatCompletionRequestMessage = API.CreateChatCompletionRequestMessage;\n  export import ChatCompletionCreateParams = API.ChatCompletionCreateParams;\n  export import CompletionCreateParams = API.CompletionCreateParams;\n  export import ChatCompletionCreateParamsNonStreaming = API.ChatCompletionCreateParamsNonStreaming;\n  export import CompletionCreateParamsNonStreaming = API.CompletionCreateParamsNonStreaming;\n  export import ChatCompletionCreateParamsStreaming = API.ChatCompletionCreateParamsStreaming;\n  export import CompletionCreateParamsStreaming = API.CompletionCreateParamsStreaming;\n}\n"],"mappings":"AAAA;SAISA,WAAW,QAAQ,iBAAiB;AAK7C,OAAM,MAAOC,WAAY,SAAQD,WAAW;QAgB1CE,CAAAC,IACE,EAAgCC,OAChC,EAA6B;;WAE7B,IAAO,CAAAC,IAAK,oBAAK;MAGlBF,IAAA;MACF,GAAAC,OAAA;MAodDE,MAAiB,GAAAC,EAAA,GAAAJ,IAAW,CAAAG,MAAA,cAAAC,EAAA,cAAAA,EAAA;IAAX"},"metadata":{},"sourceType":"module","externalDependencies":[]}